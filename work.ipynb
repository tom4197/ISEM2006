{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ApJZlfWPucxD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6707,
     "status": "ok",
     "timestamp": 1762878170150,
     "user": {
      "displayName": "Tom",
      "userId": "01086701295190410743"
     },
     "user_tz": -480
    },
    "id": "ApJZlfWPucxD",
    "outputId": "57b92e19-ecbc-445e-a9d9-c4551cce8a60"
   },
   "outputs": [],
   "source": [
    "#!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b95fac8",
   "metadata": {
    "executionInfo": {
     "elapsed": 73,
     "status": "ok",
     "timestamp": 1762878174582,
     "user": {
      "displayName": "Tom",
      "userId": "01086701295190410743"
     },
     "user_tz": -480
    },
    "id": "7b95fac8"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import os\n",
    "import PyPDF2\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "537e6c07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1954,
     "status": "ok",
     "timestamp": 1762878178082,
     "user": {
      "displayName": "Tom",
      "userId": "01086701295190410743"
     },
     "user_tz": -480
    },
    "id": "537e6c07",
    "outputId": "c0df7c57-065c-4374-c8e3-1f17152b0962"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bobch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bobch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\bobch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bobch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bd1bc62",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1762878178089,
     "user": {
      "displayName": "Tom",
      "userId": "01086701295190410743"
     },
     "user_tz": -480
    },
    "id": "8bd1bc62"
   },
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    reader = PyPDF2.PdfReader(file_path)\n",
    "    texts = []\n",
    "    for page in reader.pages:\n",
    "      text = page.extract_text()\n",
    "      text = text.strip('')\n",
    "      text = text.replace('\\n', ' ')\n",
    "      text = re.sub(r'\\s+', ' ', text)\n",
    "      texts.append(text)\n",
    "    texts = ''.join(texts)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "290c8957",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "error",
     "timestamp": 1762878178125,
     "user": {
      "displayName": "Tom",
      "userId": "01086701295190410743"
     },
     "user_tz": -480
    },
    "id": "290c8957",
    "outputId": "18e5fb14-bf40-42f2-9209-033482fc7a4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bobch\\OneDrive\\桌面\\ISEM2006 Group Project Shared\\src\n",
      "c:\\Users\\bobch\\OneDrive\\桌面\\ISEM2006 Group Project Shared\\src\\CVBF Letter to Shareholders 2015-2017.pdf\n",
      "c:\\Users\\bobch\\OneDrive\\桌面\\ISEM2006 Group Project Shared\\src\\HIBB Letter to Shareholders 2015-2017.pdf\n",
      "c:\\Users\\bobch\\OneDrive\\桌面\\ISEM2006 Group Project Shared\\src\\HMSY Letter to Shareholders 2015-2017.pdf\n",
      "c:\\Users\\bobch\\OneDrive\\桌面\\ISEM2006 Group Project Shared\\src\\KELYA Letter to Shareholders 2015-2017.pdf\n"
     ]
    }
   ],
   "source": [
    "# import all pdfs in folder\n",
    "upper_dir = os.getcwd()\n",
    "current_dir = os.path.join(upper_dir,'src')\n",
    "print(current_dir)\n",
    "pdf_list = []\n",
    "for file in os.listdir(current_dir):\n",
    "    if file.endswith('.pdf'):\n",
    "        pdf_list.append(current_dir + \"\\\\\" + file)\n",
    "print('\\n'.join(pdf_list))\n",
    "letter_list = []\n",
    "for pdf in pdf_list:\n",
    "    text = read_pdf(pdf)\n",
    "    letter_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a1fc709",
   "metadata": {
    "executionInfo": {
     "elapsed": 1248,
     "status": "aborted",
     "timestamp": 1762878178121,
     "user": {
      "displayName": "Tom",
      "userId": "01086701295190410743"
     },
     "user_tz": -480
    },
    "id": "7a1fc709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully exported to c:\\Users\\bobch\\OneDrive\\桌面\\ISEM2006 Group Project Shared\\src\\output.csv\n"
     ]
    }
   ],
   "source": [
    "filename = current_dir + '\\\\' + 'output.csv'\n",
    "# Open the file in write mode ('w') with newline='' to handle line endings correctly\n",
    "with open(filename, 'w', newline='') as csvfile:\n",
    "    # Create a CSV writer object\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the data\n",
    "    for letter in letter_list:\n",
    "        csv_writer.writerow([letter])\n",
    "\n",
    "print(f\"Data successfully exported to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72b79d1d",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1762878178128,
     "user": {
      "displayName": "Tom",
      "userId": "01086701295190410743"
     },
     "user_tz": -480
    },
    "id": "72b79d1d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "text_csv = pd.read_csv(current_dir + '\\\\' + 'output.csv', header=None)\n",
    "data = pd.DataFrame()\n",
    "data['letter'] = text_csv.astype(str).apply(lambda x: ' '.join(x), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2ca5774",
   "metadata": {
    "executionInfo": {
     "elapsed": 902,
     "status": "aborted",
     "timestamp": 1762878178131,
     "user": {
      "displayName": "Tom",
      "userId": "01086701295190410743"
     },
     "user_tz": -480
    },
    "id": "d2ca5774"
   },
   "outputs": [],
   "source": [
    "# Word tokenizer\n",
    "text = data['letter'].tolist()\n",
    "text = ''.join(text)\n",
    "tokens = word_tokenize(text)\n",
    "# Remove punctuation from each token using regex\n",
    "cleaned_tokens = [re.sub(r'[^a-zA-Z0-9]', '', token) for token in tokens]\n",
    "cleaned_tokens = [token for token in cleaned_tokens if token]  # Remove empty tokens\n",
    "cleaned_tokens\n",
    "# Remove stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "english_stopwords = set(english_stopwords)\n",
    "\n",
    "tokens_filtered = []\n",
    "for w in cleaned_tokens:\n",
    "  if w not in english_stopwords:\n",
    "    tokens_filtered.append(w)\n",
    "# Lemmatization/Stemming\n",
    "wl = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "tokens_lemmatized = [wl.lemmatize(t) for t in tokens_filtered]\n",
    "tokens_stemmed = [ps.stem(t) for t in tokens_lemmatized]\n",
    "tokens = tokens_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "Zb581iZ-qGtd",
   "metadata": {
    "executionInfo": {
     "elapsed": 310,
     "status": "aborted",
     "timestamp": 1762878178133,
     "user": {
      "displayName": "Tom",
      "userId": "01086701295190410743"
     },
     "user_tz": -480
    },
    "id": "Zb581iZ-qGtd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e7b611e1-ae07-4aef-ac0f-93013f13ab16",
       "rows": [
        [
         "('we',)",
         "98"
        ],
        [
         "('year',)",
         "98"
        ],
        [
         "('continu',)",
         "74"
        ],
        [
         "('custom',)",
         "72"
        ],
        [
         "('growth',)",
         "68"
        ],
        [
         "('store',)",
         "66"
        ],
        [
         "('new',)",
         "63"
        ],
        [
         "('busi',)",
         "60"
        ],
        [
         "('million',)",
         "60"
        ],
        [
         "('kelli',)",
         "53"
        ],
        [
         "('our',)",
         "53"
        ],
        [
         "('2016',)",
         "45"
        ],
        [
         "('increas',)",
         "43"
        ],
        [
         "('strategi',)",
         "42"
        ],
        [
         "('deliv',)",
         "40"
        ],
        [
         "('bank',)",
         "40"
        ],
        [
         "('invest',)",
         "40"
        ],
        [
         "('compani',)",
         "38"
        ],
        [
         "('oper',)",
         "35"
        ],
        [
         "('u',)",
         "34"
        ],
        [
         "('2015',)",
         "33"
        ],
        [
         "('2017',)",
         "32"
        ],
        [
         "('market',)",
         "31"
        ],
        [
         "('talent',)",
         "31"
        ],
        [
         "('fiscal',)",
         "31"
        ],
        [
         "('solut',)",
         "30"
        ],
        [
         "('also',)",
         "30"
        ],
        [
         "('sharehold',)",
         "29"
        ],
        [
         "('result',)",
         "29"
        ],
        [
         "('strong',)",
         "28"
        ],
        [
         "('expand',)",
         "27"
        ],
        [
         "('plan',)",
         "26"
        ],
        [
         "('improv',)",
         "26"
        ],
        [
         "('earn',)",
         "26"
        ],
        [
         "('health',)",
         "26"
        ],
        [
         "('share',)",
         "26"
        ],
        [
         "('in',)",
         "25"
        ],
        [
         "('revenu',)",
         "25"
        ],
        [
         "('manag',)",
         "25"
        ],
        [
         "('execut',)",
         "25"
        ],
        [
         "('focu',)",
         "24"
        ],
        [
         "('work',)",
         "24"
        ],
        [
         "('provid',)",
         "24"
        ],
        [
         "('opportun',)",
         "24"
        ],
        [
         "('initi',)",
         "23"
        ],
        [
         "('sale',)",
         "23"
        ],
        [
         "('as',)",
         "23"
        ],
        [
         "('futur',)",
         "22"
        ],
        [
         "('financi',)",
         "22"
        ],
        [
         "('total',)",
         "22"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 1563
       }
      },
      "text/plain": [
       "we          98\n",
       "year        98\n",
       "continu     74\n",
       "custom      72\n",
       "growth      68\n",
       "            ..\n",
       "height       1\n",
       "heighten     1\n",
       "here         1\n",
       "heritag      1\n",
       "york         1\n",
       "Length: 1563, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tokens).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dMwByJezsqr7",
   "metadata": {
    "executionInfo": {
     "elapsed": 6027,
     "status": "aborted",
     "timestamp": 1762878139846,
     "user": {
      "displayName": "Tom",
      "userId": "01086701295190410743"
     },
     "user_tz": -480
    },
    "id": "dMwByJezsqr7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
